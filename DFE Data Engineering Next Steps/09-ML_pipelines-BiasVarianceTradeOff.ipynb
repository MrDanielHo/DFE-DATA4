{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Variance Trade Off \n",
    "\n",
    "Linear regression models - work well with datasets that are described by a number of rows larger than the set of features (columns).<br>\n",
    "They do not work well when the number of features is greater than the number of rows.<br>\n",
    "When this happens, the Ordinary Least Squares (OLS) model does not perform well.<br>\n",
    "Therefore we need different models such as the Regularised model.\n",
    "\n",
    "## What is the Bias Variance Trade-off?\n",
    "\n",
    "Bias is defined as the difference between the models average prediction and the true population value we aim to predict.<br>\n",
    "A model with high vias pays little atention to training data and oversimplifies the model.<br>\n",
    "Leading to a lot of errors on the training and test data.\n",
    "\n",
    "When we have bias models, they tend to be very simple meaning we arrive at extreme regularisation.\n",
    "\n",
    "Underfitting<br>\n",
    "An extreme simple model oversimplifies the reality since it does not learn anything about the underlying data. \n",
    "\n",
    "Overfitting<br>\n",
    "A complex model pays too much attention to the training data but does not generalise well to new unseen data.<br>\n",
    "These models can perform well on training data, but not on testing data and results in high errors known as overfitting.\n",
    "\n",
    "## Why we choose the Mean Squared Error (MSE) in Ordinary Least Squares?\n",
    "\n",
    "When we fit an OLS model we want to minimise an objective function based on the MSE.<br>\n",
    "We choose MSE in OLS because:\n",
    "\n",
    "<ol>\n",
    "<li>Mathematically - MSE can be written as the sum of the variance and the bias:</li>\n",
    "\n",
    "$ MSE (\\hat{\\theta}) = Var _{\\theta} (\\hat{\\theta}) + Bias (\\hat{\\theta}, \\theta)^{2} $\n",
    "\n",
    "<p>OLS only focuses on the variance since the bias is always zero, therefore focuses only on how well the model fits i.e. variance.</p>\n",
    "<br></br>\n",
    "<li>Statistically - MSE allows interpretation of variance to measure the accuracy of your model.</li>\n",
    "<p>Think about bias in terms of precision - high bias means nonprecise model.<p>\n",
    "<p>They will not learn anything about training data therefore we can treat variance like accuracy.\n",
    "</ol>\n",
    "\n",
    "# Take home messages\n",
    "<ol>\n",
    "<li>We have to find a good balance between overfitting and underfitting.</li>\n",
    "<br></br>\n",
    "<li>The optimal fitting is a model that is nmot too biased, but tries to generalise with respect to all data.</li>\n",
    "<p>&nbsp;<i>We have to take care between the trade off between bias and variance.</i></p>\n",
    "<br></br>\n",
    "<li>One reason for extending a linear regression model is to add bias and this is why we use regularised models.</li>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
