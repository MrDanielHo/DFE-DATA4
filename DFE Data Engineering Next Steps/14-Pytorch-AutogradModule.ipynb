{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd Module\n",
    "`torch.autograd` is PyTorchâ€™s automatic differentiation engine that powers neural network (NN) training.\n",
    "\n",
    "NNs are a collection of nested functions that are executed on some input data.<br>\n",
    "These fucntions are defined by *parameters* (consisting of weights and biases), which are stored in tensors.\n",
    "\n",
    "Training a NN happens in two steps:\n",
    "\n",
    "**Forward Propagation**: In forward prop, NN makes the best guess about the correct output.<br>\n",
    "&emsp;It runs data through each of its functions to make this guess.\n",
    "\n",
    "**Backward Propagation**: In backprop, NN adjusts the parameters proportionate to teh error in its guess.<br>\n",
    "&emsp;It does by traversing backwards from the output, collection the derivatives of the error with respect to the parameters of the functions (gradients), and optimising the parameters using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([-1.2365, 1.8245, -1.0701, 0.0869, -0.0376])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = x**2 -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.4711, -1.6712, -3.8549, -4.9924, -4.9986])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is applying the loss function operations element-wise to tensor x.<br>\n",
    "We can say that this is the forward propagation step.<br>\n",
    "Given the input, we get an output based on a loss function.\n",
    "\n",
    "For PyTorch to compute the gradients, we have to set the tensor argument `requires_grad=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([-1.2365, 1.8245, -1.0701, 0.0869, -0.0376], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument `requires_grad=True` all operations on the tensor are tracked in the computational graph.<br>\n",
    "Every operation on the tensor is tracked by the PyTorch backend for us.\n",
    "\n",
    "PyTorch uses a dynamic computation graph versus Tensorflow using a static computation graph.<br>\n",
    "It is worth noting in PyTorch is dynamic because when we create any operation involving a tensor, it is executed immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = x**2 -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.4711, -1.6712, -3.8549, -4.9924, -4.9986], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SubBackward0 at 0x24100f72d00>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the gradients, we need to apply the backward fucntion to the loss.<br>\n",
    "As we are expecting a non-scalar output, we have to specify the gradient argument inside the backward call, which is simply a tensor with the same shape as tensor *x*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward(gradient=torch.ones(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this operation we allow the backward method to perform the vector Jacobian product to get the gradients.<br>\n",
    "Hence the backward method computes for us the gradient with respect to the tensor *x*, which is then accumulated into the `grad` attribute.<br>\n",
    "This takes into account the partial derivative of the function with respect to the tensor's elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.4730,  3.6490, -2.1402,  0.1738, -0.0752])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check PyTorch is correctly computing the gradients, we can compute the algebriac derivative of the loss with respect to x.\n",
    "\n",
    "PyTorch is automatically computing the following partial derivative, namely the partial derivative of the loss with respect to tensor x.<br>\n",
    "This is nothing more than two times x:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x} = 2x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.4730,  3.6490, -2.1402,  0.1738, -0.0752], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `autograd` package keeps track of all tensors' operations, along with the resulting new tensors, in a Directed Acyclic Graph (DAG).<br>\n",
    "In this DAG, leaves are the input tensors and roots are the output tensors.<br>\n",
    "This concept can be extracted from the previous graph.\n",
    "\n",
    "This is a nice feature, but not recommended when input data and/or the Computational Graph structuire is too complicated.<br>\n",
    "So it is good practice to prevent PyTorch from keeping track of all the gradients' history inside the DAG.\n",
    "\n",
    "For example, when training a Neural Network.<br>\n",
    "In a backpropagation step, you update the weights on the net, and this operation should not be part of the gradient computation.<br>\n",
    "How can we prevent PyTorch from keeping the history of gradients?<br>\n",
    "There are multiple ways but we will look at two methods.\n",
    "\n",
    "## Using detach\n",
    "Detach creates a new tensor with the same values but does not require the gradient.<br>\n",
    "When we compute the loss, PyTorch will not create a gradient function to be stored inside the DAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2,2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.rand(2,2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    loss =c ** 2\n",
    "    print(loss.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
