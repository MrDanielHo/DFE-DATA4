{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute\n",
    "Learning objectives:\n",
    "* Elastic Compute Cloud (EC2);\n",
    "* The Elastic Container Service (ECS);\n",
    "* The Elastic Container Registry (ECR);\n",
    "* The Elastic Container Service for Kubernetes (EKS);\n",
    "* AWS Elastic Beanstalk;\n",
    "* AWS Lambda;\n",
    "* AWS Batch;\n",
    "* Amazon Lightsail;\n",
    "* Concept of:\n",
    "    * Elastic Load Balancing\n",
    "    * EC2 Auto Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Compute in AWS?\n",
    "Compute resources can be considered the brains and processing power required by applications and systems to carry out computational tasks via a series of instructions.\n",
    "\n",
    "Compute is closely related to common server components such as CPUs and RAM.\n",
    "\n",
    "A physical server within a data center would be considered a Compute resource as it may have multiple CPUs and many Gigabytes of RAM.\n",
    "\n",
    "Within AWS, there are a number of different services and features that offer Compute power to provide different functions:\n",
    "\n",
    "&emsp;Compute services can comprise of utilising hundreds of EC2 instances (virtual servers) which may be used continuously for months or even years processing millions of instructions.\n",
    "\n",
    "&emsp;You may only utilise a few hundred milliseconds of compute resource to execute just a few lines of code within AWS Lambda before relinquishing that compute power.\n",
    "\n",
    "Compute resources can be consumed in different quantities, for different lengths of time, across a range of categories, offering a wide scope of performance and benefit options.<br>\n",
    "The compute resource will be highly depend on use case as to which Compute resource should be used within AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Compute Cloud (EC2)\n",
    "EC2 allows the deployment of virtual servers within an AWS environment.<br>\n",
    "Most people will require an EC2 instance within their environment as part of at least one of their solutions.\n",
    "\n",
    "The EC2 service can be broken down into the following components:\n",
    "* Amazon Machine Images (AMIs),\n",
    "* Instance types,\n",
    "* Instance Purchasing OPtions,\n",
    "* Tenancy,\n",
    "* User Data,\n",
    "* Storage options, and\n",
    "* Security\n",
    "\n",
    "## Amazon Machine Images (AMI)\n",
    "AMIs are essentially templates of preconfigured EC2 instances which allow you to quickly launch a new EC2 instance based on the configuration within the AMI.\n",
    "\n",
    "An AMI is an image baseline with an operating system and applications alongwith any custom configuration.\n",
    "\n",
    "You may want to create your own AMI to speed your own deployments.<br>\n",
    "You do this by:\n",
    "1. Select and launch an AWS AMI e.g. Linux Server\n",
    "2. Once the EC2 instance is running, configure and install applications for this instance.\n",
    "3. If we needed another server to perform the same functionality, we could go through the same process (steps 1-3) or we could:\n",
    "4. Create a template of the instance that contains all the customisations that was made previously and deploy as many as needed.\n",
    "\n",
    "### AWS Marketplace\n",
    "AWS Marketplace is an online store where vendors sell their AMIs.<br>\n",
    "Such vendors include Cicso, Citrix, and Alert Logic.\n",
    "\n",
    "Vendor AMIs may have specific applications and configurations premade, such as instances that are optimised with built-in security and monitoring tools or contained database migrations systems.\n",
    "\n",
    "### Community AMIs\n",
    "Community AMIs is a repositotu of AMIs that have been created and shared by other AWS members.\n",
    "\n",
    "## Instance Types\n",
    "Once you have selected your AMI from any of the different sources, you must then select an instance type.\n",
    "\n",
    "An instance type defines the size of the instance based on a number of different parameters:\n",
    "* ECUs - the number of EC2 compute units for the instance\n",
    "* **vCPUs** - the number of virtal CPUs on the instance\n",
    "* Physical processor - the underlying processor used on the instance\n",
    "* Clock speed\n",
    "* **Memory**\n",
    "* **Instance storage** - capacity of the local instance store volumns available\n",
    "* EBS Optimised Storage - defines if the instance supports EBS oprimised storage or not\n",
    "* **Network performance** - performance level of rate of data transfer\n",
    "* IPv6 support\n",
    "* Prcoessor Architecture\n",
    "* AES-NI (Advanced Encryption Standard - New Instructions) - if the instance supports it for enchanced data protection\n",
    "* AVX - support for advanced vector extensions, used for audio, video, scientific calculations, and 3D modeling analysis.\n",
    "* Turbo - does the instance support intel turbo boost or AMD turbo core technologies.\n",
    "\n",
    "The key parameters to look our for are vCPU, Memory, Instance Storage, and Network Performance.<br>\n",
    "The use case and application will determine which type should be deployed.\n",
    "\n",
    "The different instance types are categorised into different family types that offer distinct performance benefits:\n",
    "* Micro instances - low cost, minimal CPU and memory power\n",
    "* General purpose - mix of CPU memory and storage, ideal for small-medium databases, tests and development servers, and back-end servers\n",
    "* Compute optimised - high performance processes installed allowing use for high-performance front end servers, web servers, high-performance science and engineering applications, video encoding and batch processing\n",
    "* FPGA instances - Field Programmable Gate Arrays, programmed to create application specific hardware accelerations, with high CPU performance, large memory, and high network bandwidth for applications requiring parallel processing power.\n",
    "* GPU instances - Graphics Processing Unit, used with graphic intensive applications\n",
    "* Memory optimised - primarily used for large-scale enterprise class in-memory applications, such as performing real time processing of unstructured data. Also ideal for enterprise applications such as Microsoft SharePoint.\n",
    "* Storage optimised - SSD backed instant sroarage for low latency and very high input/output (I/O) performance, including very high input/output operations per second (IOPS). Ideal use for analytic workloads and no SQL databases, data file systems, and log processing applications.\n",
    "\n",
    "## Instance Purchasing Options\n",
    "EC2 instances can be purchased through a number of payment plans:\n",
    "* On-demand instances\n",
    "    * Can be launched at any time,\n",
    "    * Can be used for as long as necessary\n",
    "    * Flat rate deteremined on the instance type\n",
    "    * Typically short-term uses\n",
    "    * Best fit for testing and development environments\n",
    "* Reserved instances\n",
    "    * Purchased for a set period of time for discount up to 75%\n",
    "    * All upfront - complete payment for 1 or 3 year time frame provides the largest discount\n",
    "    * Partial upfront - smaller upfront payment for smaller discount\n",
    "    * No upfront - smallest discount is applied\n",
    "    * Best applied for long term, predictable workloads\n",
    "* Scheduled instances\n",
    "    * Pay for the reservations on a recurring schedule e.g. daily, weekly, monthly\n",
    "    * Set up a scheduled instance to run during a set time frame once a week\n",
    "    * Note that even if you didn't use the instance you would still be charged.\n",
    "* Spot instances\n",
    "    * Bid for unused EC2 compute resources\n",
    "    * No guarantees for a fixed period of time\n",
    "    * Fluctuation of prices based on supply and demand\n",
    "    * Purchase large EC2 instances at a very low price\n",
    "    * Useful for processing data that can be suddenly interrupted\n",
    "* On-demand capactity reservations\n",
    "    * Reserve capacity based on different attributes such as instance type, platform, and tenancy, within a particular availability zone for any perioud of time\n",
    "    * It could be used in conjuction with your reserved instances discount\n",
    "\n",
    "## Tenancy\n",
    "This relates to what underlying host your EC2 instance will reside on, so essentially the physical server within an AWS datacenter.\n",
    "\n",
    "* Shared Tenancy \n",
    "    * EC2 instance is launched on any available host with the required resources\n",
    "    * The same host can be used by multiple customers\n",
    "    * AWS Security mechanisms prevent one EC2 instnace accessing another in the same host\n",
    "* Dedicated instances\n",
    "    * Hosted on hardware that no other customer can access\n",
    "    * May be required to meet compliance\n",
    "    * Dedicated instances inccur additional charges\n",
    "* Dedicated hosts - same features as dedicated instances as well as:\n",
    "    * Additional visibility and control on the physical host\n",
    "    * Allows to use the same host for a number of instances\n",
    "    * May be required to meet compliance\n",
    "\n",
    "## User data\n",
    "User data allows you to enter commands that will run during the first boot cycle of that instance\n",
    "* Perform functions upon boot such as to pull down any additional software you want\n",
    "* Download latest OS updates\n",
    "\n",
    "## Storage options\n",
    "Selecting storage for the EC2 instance depends on the instance selected, what you intend to use the instance for, and how critical the data is.<br>\n",
    "There are two categories:\n",
    "* Persistance storage - available by attaching Elastic Block Storage (EBS) volumes\n",
    "    * EBS volumes are separated from the EC2 instance\n",
    "    * These columes are logically attached via AWS network\n",
    "    * Disconnecting the volumn from the EC2 instance maintains the data\n",
    "    * Possible to implement encryption and take backup snapshots of all data\n",
    "* Ephemeral storage - created by EC2 instances using local storage\n",
    "    * Physically attached to the underlying host\n",
    "    * When instance is terminated, all saved data on disk is lost\n",
    "    * Rebooting - data will remain intact\n",
    "    * Unable to detach instance store volumes from the instance\n",
    "\n",
    "## Security\n",
    "During the creation of the EC2 instance you will be asked to select a Security Group for your instance.\n",
    "\n",
    "At the end of the EC2 instance creation, you will need to select an existing Key Pair or create and download a new one.\n",
    "\n",
    "A Key pair is made up of a Public Key and a Private Key.<br>\n",
    "The function of Key Pairs is to encrypt the login information for Linux and Windows EC2 instances, and then decrypt the same information allowing you to authenticate onto the instance.\n",
    "\n",
    "The Public Key is used to login with the username and password.\n",
    "\n",
    "The Private Key decrypts this data allowing you to gain access to the login credentials (Windows) or to remotelu connect onto the instance via SSH (Linux).\n",
    "\n",
    "The Public Key is held and kept by AWS, the Private Key is the responsibility of the user to keep and ensure it is not lost.\n",
    "\n",
    "It is possible to use the same Key Pair on multiple instances.\n",
    "\n",
    "You can set up additional less priviledged access controls, such as local Windows accounts.\n",
    "\n",
    "It is the reponsibility of the user to maintain and install the latest OS and security patches released by the OS vendor as dictated within the AWS shared responsibility model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EC2 Container Service (ECS)\n",
    "This service allows the use of Docker-enabled applications packaged as containers across a cluster of EC2 instances without requiring you to manage a complex and administrativelty heavy cluster management system.\n",
    "\n",
    "The burden of managing your own cluster management system is abstracted with the ECS service by passing that responsibility over to AWS specifically through the use of the AWS Fargate.\n",
    "\n",
    "## AWS Fargate\n",
    "AWS Fargate is an engine used to enable ECS to run containers without having to manage and provision instances and clusters for containers.\n",
    "\n",
    "## Docker\n",
    "Docker is a piece of software that allows you to automate the installation and distributions of applications inside Linux Containers.\n",
    "\n",
    "## Containers\n",
    "Containers hold everything an applications needs to run from within its container package.\n",
    "\n",
    "They are decoupled from the operating system, making Container applications very portable.\n",
    "\n",
    "## ECS\n",
    "ECS removes the need for you to manage your own cluster management system thanks to its interaction with AWS Fargate.\n",
    "\n",
    "With Amazon ECS there is no need to install any management or monitoring software for the cluster.\n",
    "\n",
    "## Launching an ECS cluster\n",
    "When launching an ECS cluster, there are 2 deployment modes:\n",
    "\n",
    "### Fargate Launch\n",
    "Requires you to specify the CPU and memory required, define networking and IAM policies, in addition to you having to package your application into containers.\n",
    "\n",
    "### EC2 Launch\n",
    "You are responsible for patching and scaling your instances and you can specify instance type and how many containers should be in a cluster.\n",
    "\n",
    "## Monitoring Containers\n",
    "Monitoring is taken care of through the use of Amazon CloudWatch.\n",
    "\n",
    "You can easily create alarms based off these metrics, providing you notification of when specific events occur, such as a your cluster size scaling up or down.\n",
    "\n",
    "## Amazon ECS Cluster\n",
    "An Amazon ECS cluster is comprised of a collection of EC2 instances.\n",
    "\n",
    "Features such as Securtiy Groups, Elastic Load Blanacing, and Auto Scaling can be used with these instances.\n",
    "\n",
    "These instances still operate in much the same way as a single EC2 instance.\n",
    "\n",
    "Clusters act as a resource pool, aggregating resources such as CPU and memory\n",
    "\n",
    "Clusters are dynamically scalable and multiple instances can be used.\n",
    "\n",
    "Clusters can only scale in a single region.\n",
    "\n",
    "Containers can be scheduled to be deployed across your cluster.\n",
    "\n",
    "Instances within the cluster also have a Docker daemon and an ECS agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Elastic Container Registry (ECR)\n",
    "ECR provides a secure location to store and manage your docker images.\n",
    "\n",
    "This is a fully managed service, so you do not need to provision any infrastructure to allow you to create this registry of docker images.\n",
    "\n",
    "This service allows developers to push, pull, and manage their library of docker images in a central and secure location.\n",
    "\n",
    "The components used in ECR:\n",
    "* Registry\n",
    "* Authorisation token\n",
    "* Repository\n",
    "* Repository policy\n",
    "* Image\n",
    "\n",
    "## Registry\n",
    "The ECR registry allows you to host and store your docker images in as well as create image repositories.\n",
    "\n",
    "Your account will have read and write access by default to any images you create within the registry and any repositories.\n",
    "\n",
    "Access to your registry and images can be controlled via IAM policies in adddition to repository policies.\n",
    "\n",
    "Before your docker client can access your registry lit needs to be authenticated as an AWS user via an Authorisation Token.\n",
    "\n",
    "## Authorisation Token\n",
    "To begin the authorisation process to communicate your docker client with your default registry, you can run the get-login command using the AWS CLI\n",
    "\n",
    "`aws ecr get-login --region region --no-include-email`\n",
    "\n",
    "This will produce an output response which will be a docker login command\n",
    "\n",
    "`docker login -u AWS -p password`<br>\n",
    "`https://aws_account_id.dkr.ecr.region.amazonaws.com`\n",
    "\n",
    "This process produces an authorisation token that can be used within the registry for 12 hours.\n",
    "\n",
    "## Repository \n",
    "These are objects within your registry that allow you to group together and secure different docker images.\n",
    "\n",
    "You can create multiple repositories with the registry allowing you to organise and manage your docker images into different categories.\n",
    "\n",
    "Using policies from both IAM and repository policies you can assign set permissions to each repository.\n",
    "\n",
    "## Repository Policy\n",
    "There are a number of different IAM managed policies to help you controll acces to ECR:\n",
    "\n",
    "`AmazonEC2ContainerRegistryFullAccess`<br>\n",
    "`AmazonEC2ContainerRegistryPowerUser`<br>\n",
    "`AmazonEC2ContainerRegistryReadOnly`\n",
    "\n",
    "Repository policies are resource-based policies.\n",
    "\n",
    "You need to ensure you add a principal to the policy to determine who has access and what permissions they have.\n",
    "\n",
    "For an AWS user to gain access to the registry they ill require access to the `ecr:GetAuthorizationToken` API call.\n",
    "\n",
    "Once they have this access, repository policies can control what actions those users can perform on each of the repositories.\n",
    "\n",
    "# Image\n",
    "Once you have configured your registry, repositories and security controls, and authenticated your docker client with ECR, you can then begin storing your docker images in the required repositories.\n",
    "\n",
    "To push an image into ECR, use the docker push command.\n",
    "\n",
    "To retrieve and image, use the docker pull command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Elastic Container Service for Kubernetes (EKS)\n",
    "## What is Kubernetes\n",
    "Kubernetes is an open-source container orchestration tool designed to automate, deploy, scale, and operate containerised applications.\n",
    "\n",
    "It can grow from tens, thousands, or even millions of containers.\n",
    "\n",
    "It is container-runtime-agnostic - it can be used to run RocKeT and Docker containers.\n",
    "\n",
    "## What is EKS\n",
    "AWS provides a managed service allowing you to run Kubernetes across your AWS infrastructure without having to take care of provisioning and running the Kubernetes management infrastructure in what's referred to as the control plane.\n",
    "\n",
    "You only need to provision and maintain the worker nodes.\n",
    "\n",
    "## Kubernetes control plane\n",
    "There are a number of components that make up the control plane and these include a number of different APIs, the kubelet processes and the Kubernetes Master.\n",
    "\n",
    "The control plane schedules containers onto nodes.<br>\n",
    "Scheduling refers to the decision process of placing containers onto nodes in accordance with their declared compute requirements.\n",
    "\n",
    "The control plane also tracks the state of all Kubernetes objects by continually monitoring the objects.\n",
    "\n",
    "In EKS, AWS is responsibile for provisioning, scaling, and managing the control plane, and they do this by utilising multiple availability zones for additional resilence.\n",
    "\n",
    "## Worker nodes\n",
    "Kubernetes clusters are composed of nodes.\n",
    "\n",
    "A node is a worker machine in Kubernetes.<br>\n",
    "It runs as an on-demand EC2 instance and includes software to run containers.\n",
    "\n",
    "For Each node created, a speicific AMI is used, which also ensures Docker and the kubelet is installed for security controls.\n",
    "\n",
    "Once the worker nodes are provisioned they can then connect to EKS using an endpoint.\n",
    "\n",
    "## Working with EKS\n",
    "1. Create an EKS Service Role\n",
    "    * Before you begin working with EKS you need to configure and create an IAM service role that allows EKS to provision and confidure specific resources.\n",
    "    * The role needs to have the following permissions policies attached to the role:\n",
    "        * AmazonEKSServicePolicy\n",
    "        * AmazonEKSClusterPolicy\n",
    "2. Create an EKS Cluster VPC\n",
    "    * Create and run a CloudFormation stack based on the template below, which will configure a new VPC for you to use with EKS.\n",
    "3. Install kubectl and the AWS-IAM-Authenticator\n",
    "    * Kubectl is a command line utility for Kubernetes.\n",
    "    * The IAM-Authenticator is require to authenticate with the EKS cluster.\n",
    "4. Create an EKS Cluster\n",
    "    * You can now create your EKS cluster using the details and information from the VPC created in step 1 and 2.\n",
    "5. Configure kubectl for EKS\n",
    "    * Using the `update-kubeconfig` command via the AWS CLI you need to create a kubeconfig file for your EKS cluster.\n",
    "6. Provision and configure Worker Nodes\n",
    "    * Once the EKS ccluster shows an 'Active' status you can launch your worker nodes using CloudFormation based on the template.\n",
    "7. Configure the Worker Node to join the EKS Cluster\n",
    "    * Using a oconfiguration map, you must replace the `<ARN of instance role (not instance profile)>` with the NodeInstanceRole value from step 6.\n",
    "\n",
    "Your EKS Cluster and worker nodes are now configured ready to deploy your applications with Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Elastic Beanstalk\n",
    "AWS Elastic Beanstalk is an AWS managed service that takes your uploaded code of your web application code and automatically provisions and deploys the required resources within AWS to make the web application operational.\n",
    "\n",
    "These resources include EC2, Auto Scaling, application health-monitoring and Elastic Load Balancing, in addition to capacity provisioning.\n",
    "\n",
    "An ideal service for engineers who may not have familiarity or the necessary skills within AWS to deploy, provision, monitor andd scale the correct environment to run the developed applications.\n",
    "\n",
    "The responsibility is passed on to AWS Elastic Beanstalk to deploy the correct infrastructure to run the uploaded code.\n",
    "\n",
    "You can continue to support and maintain the environment as you would with a custom built environment.\n",
    "\n",
    "You can perform some maintenance tasks from the Elastic Beanstalk dashboard itself.\n",
    "\n",
    "AWS Elastic Beanstalk is able to operate with a variety of platforms and programming languages:\n",
    "* Pakcer builder\n",
    "* Single container docker\n",
    "* Multicontainer docker\n",
    "* Preconfigured docker\n",
    "* Go\n",
    "* Java SE\n",
    "* Java with Tomcat.NET on Windows Server with IIS\n",
    "* Node.js\n",
    "* PHP\n",
    "* Python\n",
    "* Ruby\n",
    "\n",
    "The service itself is free to use.<br>\n",
    "There is no cost associated with Elastic Beanstalk, however any resources that are created on the application's behalf, such as EC2 instances, you will be charged for as per the standard pricing policies at the time of deployment.\n",
    "\n",
    "## Elastic Beanstalk core components\n",
    "### Application verion\n",
    "An application version is a very specific resource to a section of deployable code.\n",
    "\n",
    "The application version will point typically to S3, simple storage service to where the deployable code may reside.\n",
    "\n",
    "### Environment\n",
    "An environment refers to an application version that has been deployed on AWS resources, which are configured an provisioned by AWS Elastic Beanstalk. \n",
    "\n",
    "At this stage, the application is deployed as a solution and becomes operational within your environment.\n",
    "\n",
    "The environment is comprised of ALL the resources created by Elastic Beanstalk and not just an EC2 instance with your uploaded code.\n",
    "\n",
    "### Environment configurations\n",
    "This is a collection of parameters and settings that dictate how an environment will have its resources provisioned by Elastic Beanstalk and how these resources will behave.\n",
    "\n",
    "### Environment tier\n",
    "If the application manages and handels HTTP requests then the app will be run in a web server environment.\n",
    "\n",
    "If the application does not process HTTP requests, and instead perhaps pulls data from an SQS queue, then it would run in a worker environment.\n",
    "\n",
    "### Configuration template\n",
    "This is the template that provides the baseline for creating a new, unique, environment configuration.\n",
    "\n",
    "### Platform\n",
    "Platform is a culmination of components in which you can build your application upon using Elastic Beanstalk.\n",
    "\n",
    "These comprise of the operating system of the instance, the programming language, the server type (web or application), and components of Elastic Beanstalk itself, and as a whole can be defined as a platform.\n",
    "\n",
    "### Applications\n",
    "An application is a collection of different elements, such as environments, environment configurations and application versions.\n",
    "\n",
    "#### Web Server Environment\n",
    "This is typically used for standard web applications that operate and serve requests over HTTP port 80:\n",
    "* Route 53\n",
    "* Elastic Load Balancer\n",
    "* Auto Scaling\n",
    "* EC2\n",
    "* Security Groups\n",
    "\n",
    "#### Worker Environment \n",
    "It is used by applications that will have a back-end processing task, that will interact with AWS SQS:\n",
    "* SQS Queue\n",
    "* IAM Service Role\n",
    "* Auto Scaling\n",
    "* EC2\n",
    "\n",
    "## Elastic Beanstalk Workflow\n",
    "AWS Elastic Beanstalk operates a very simple workflow process for your application deployment and ongoing management.\n",
    "\n",
    "1. Create application\n",
    "2. Upload version\n",
    "    * Upload application version\n",
    "    * Upload additional configuration information\n",
    "3. Launch environment\n",
    "4. Manage Environment\n",
    "    * Deploy new versions of the application\n",
    "    * Changes in the environment configuration will automatically update the environment should new code require additional resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Lambda\n",
    "AWS Lambda is a serverless compute service that allows you to run your application code without having to manage EC2 instances.\n",
    "\n",
    "Serverless means that you do not need to provision or manage the compute resources to run the code, instead this is managed and provisioned by AWS.\n",
    "\n",
    "The service does not require compute power to carry out the code requests, but because the AWS user does not need to be concerned with what managing this compute power or where its provisioned from, it's considered serverless from the user perspective.\n",
    "\n",
    "You only ever have to pay for compute power when Lambda is in use via Lambda Functions.\n",
    "\n",
    "AWS Lambda charges compute power per 100ms of use only when running code, in addition to the number of times your code runs.\n",
    "\n",
    "## Working with AWS Lambda\n",
    "There are 4 essential steps:\n",
    "1. Upload code to Lambda, or write it within the code editors provided by Lambda.\n",
    "    * Languages supported: \n",
    "        * Node.js\n",
    "        * Java\n",
    "        * C#\n",
    "        * Python\n",
    "        * Go\n",
    "        * Powershell\n",
    "        * Ruby\n",
    "2. Configure Lambda functions to execute upon triggers from supported event sources.\n",
    "    * E.g. An object is uploaded to an S3 bucket\n",
    "3. Once the specific trigger is initiated, Lambda will run the code (as per the Lambda function) using only the require compute power as defined.\n",
    "4. AWS recourds the compute time in milliseconds and the quantity of Lambda functions run to ascertain the cost of the service.\n",
    "\n",
    "## Componenets of AWS Lambda\n",
    "The following elements form the key constructs of a Lambda Application:\n",
    "* The Lambda function is complied of your own code that you want Lambda to invoke as per defined triggers.\n",
    "* Events sources are AWS services that can be used to trigger your Lambda functions.\n",
    "* A Triggetr is essentially an operation from an even source that causes the function to invoke.\n",
    "* Downstream resources are resources that are required during th execution of your Lambda Function.\n",
    "* Log streams help to identify issues and troubleshoot issues with your Lambda function.<br>\n",
    "  These log streams would essentially be a sequence of events that all come from the same function and recorded in CloudWatch.\n",
    "\n",
    "## Creating Lambda functions\n",
    "At a high level, the configuration steps for creating a Lambda function via the AWS Management console could consist of:\n",
    "1. Selecting a Blueprint\n",
    "  * Select a blueprint template provided by AWS Lambda.\n",
    "  * E.g. *S3-get-object* - an Amazon S3 trigger that retrieves metadata for the object that is being updated.\n",
    "2. Configure Triggers\n",
    "  * Define the triggers for your Lambda function.\n",
    "  * E.g. specifying the S3 bucket for your function.\n",
    "3. Configure Function\n",
    "  * Upload code or edit in-line\n",
    "  * Define the required resources maximum execution timeout, IAM Role and Handler Name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Batch\n",
    "AWS Batch is used to manage and run batch computing workloads within AWS.\n",
    "\n",
    "## What is batch computing\n",
    "Batch Computing is primarily used in specialist use cases which require a vast amount of compute power across a cluster of coumpute resources to complete batch processing executing a series of tasks.\n",
    "\n",
    "With AWS Batch many of these constraints, administration activites and maintenance tasks are removed.\n",
    "\n",
    "You can seamlessly create a cluster of compute resources which is highly scalable taking advanrage of the elasticity of AWS coping with any level of batch processing whilst optimising the distribution of the workloads.\n",
    "\n",
    "All provisioning, monitoring, maintenance and management of the clusters is taken care of by AWS.\n",
    "\n",
    "## AWS Batch Components\n",
    "1. Jobs<br>\n",
    "A Job is classed as the unit of work that is to be run by AWS Batch.\n",
    "    * A job can be executable file, an application within an ECS Cluster or a shell script.\n",
    "    * Jobs run on EC2 Instances as a containerised application.\n",
    "    * Jobs can have different states such as Submitted, Pending, Running, Failed, etc.\n",
    "2. Job Definitions<br>\n",
    "These define specific parameters for the Jobs themselves and dictate how the Job will run and with what configuration.<br>\n",
    "For example:\n",
    "    * How many vCPUs to use for the container\n",
    "    * Which data volumnes should be used\n",
    "    * Which IAM role should be used to allow access for AWS Batch to communicate with other AWS services\n",
    "    * Mount points\n",
    "3. Job Queues<br>\n",
    "Jobs that are scheduled are placed into a Job Queue until they are run.\n",
    "    * You can have multiple queues with different priorities\n",
    "    * On-demand and Spot instances are supported\n",
    "    * AWS Batch can bid on your behalf for Spot instances\n",
    "4. Job Scheduling<br>\n",
    "The Job Scheduler takes care of when a Job should be run and from which Compute Environment.\n",
    "    * Typically it will operate on a first-in-first-out basis\n",
    "    * It ensures that highter priority queues are run first\n",
    "5. Compute Environments<br>\n",
    "These are the environments containing the compute resources to carry out the Job\n",
    "    * Managed Environments\n",
    "        * The service will handle provisioning, scaling, and termination of Compute instances\n",
    "        * This environment is created as an Amazon ECS Cluster\n",
    "    * Unmanaged Environments\n",
    "        * These environments are provisioned, managed, and maintained by you\n",
    "        * It gives greater customisation but requires greater administration and maintenance\n",
    "        * It requires you to create the necessary Amazon ECS Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Lightsail\n",
    "Amazon Lightsail is essentially a Virtual Private Server (VPS) backed by AWS infrastructure, much like an EC2 instance but without as many configurable steps throughout its creation.\n",
    "\n",
    "It has been disigned to be simple, quick, and very easy to use at a low cost point, for small scale use cases by small businesses or single users.\n",
    "\n",
    "It is commonly used to host simple websites, small applications, and blogs.\n",
    "\n",
    "You can run multiple Lightsail instances together allowing them to communicate.\n",
    "\n",
    "It is possible to connect it to other AWS resources and to your existing VPC running within AWS via a peering connection.\n",
    "\n",
    "## Deploying a Lightsail instance\n",
    "\n",
    "A Lightsail instance can be deployed from a single page with just a few simple configuration options.\n",
    "\n",
    "Amazon Lightsail can be accessed either via the AWS console under the Compute category, or directly through the homepage of AWS Lightsail which sits outside of the AWS Management Console.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
